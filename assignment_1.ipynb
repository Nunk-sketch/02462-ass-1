{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color: #E6E6E6;\">\n",
    "<span style=\"background-color: #00695C; padding:8px; display:block; border-left:4px solid #4682b4\">\n",
    "\n",
    "# Assignment 1 - CNN's and VGG16\n",
    "\n",
    "*In this assingment, you will further familiraize yourself with CNN's and how to implement them. For this particular example, we will ask you to implement the layer structure of VGG16, an old but fairly effective and simple CNN structure.*\n",
    "\n",
    "*Keep in mind, that while VGG16 and other CNN's you have implemented so far, only incoporate convolutions and pooling layers, many state-of-the-art models  use a variety of other techniques, such as skip connections, or self-attention to get better results.*\n",
    "\n",
    "*As you write code for this assignment, try to keep in mind to write good code. That might sound vague, but just imagine that some other poor sod will have to read your code at some point, and easily readable, understandable code, will go a long way to making their life easier. However, this is not a coding course, so the main focus should of course be on the exercises themselves.*\n",
    "\n",
    "**Keep in mind, this assignment does not count towards your final grade in the course. When any of the exercises mention 'grading', it refers to commenting and correcting answers, not necessarily giving you a score which will reflect in your grade, so dw :)**\n",
    "\n",
    "\n",
    "**Hand-in date is 30/09 at the latest if you want to recieve feedback!!**\n",
    "\n",
    "</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "## Boilerplate start - you can mostly ignore this!"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "import os\n",
    "import torch\n",
    "import PIL\n",
    "from torch import nn\n",
    "from torch.utils.data.dataloader import default_collate\n",
    "\n",
    "import torchvision.datasets as datasets\n",
    "from torchvision.transforms import ToTensor\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.models\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "\n",
    "import utils\n",
    "\n",
    "# Check if you have cuda available, and use if you do\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "\n",
    "# Set a random seed for everything important\n",
    "def seed_everything(seed: int):\n",
    "    import random, os\n",
    "    import numpy as np\n",
    "    import torch\n",
    "    \n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = False\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "# Set a seed with a random integer, in this case, I choose my verymost favourite sequence of numbers\n",
    "seed_everything(sum([115, 107, 105, 98, 105, 100, 105, 32, 116, 111, 105, 108, 101, 116]))"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Specify dataset you wanna use\n",
    "def get_dataset(dataset_name, validation_size=0.1, transform=None, v=True, imagenette_resize_size=256, imagenette_crop_size=224):\n",
    "\n",
    "    if transform is None:\n",
    "        transform = ToTensor()\n",
    "\n",
    "    if dataset_name == 'cifar10':\n",
    "        train_set = datasets.CIFAR10(root='./data', train=True, download=True, transform=ToTensor())\n",
    "        test_set = datasets.CIFAR10(root='./data', train=False, download=True, transform=ToTensor())\n",
    "\n",
    "        # Purely for our convenience - Mapping from cifar labels to human readable classes\n",
    "        cifar10_classes = {\n",
    "            0: 'airplane',\n",
    "            1: 'automobile',\n",
    "            2: 'bird',\n",
    "            3: 'cat',\n",
    "            4: 'deer',\n",
    "            5: 'dog',\n",
    "            6: 'frog',\n",
    "            7: 'horse',\n",
    "            8: 'ship',\n",
    "            9: 'truck'\n",
    "        }\n",
    "\n",
    "    elif dataset_name == 'mnist':\n",
    "        train_set = datasets.MNIST(root='./data', train=True, download=True, transform=ToTensor())\n",
    "        test_set = datasets.MNIST(root='./data', train=False, download=True, transform=ToTensor())\n",
    "\n",
    "    elif dataset_name == 'imagenette':\n",
    "        download = not os.path.exists('./data/imagenette2')\n",
    "\n",
    "        # Specific transform in the case we use imagenette\n",
    "        imagenette_transform = transforms.Compose([\n",
    "            transforms.Resize(imagenette_resize_size),        # Resize the image to a default value of 256x256 (what the VGG paper does)\n",
    "            transforms.RandomCrop(imagenette_crop_size),        # Crop the center to a default value of 224x224 (what the VGG paper does)\n",
    "            transforms.ToTensor(),         # Convert to tensor\n",
    "            transforms.Normalize(mean=[0.4650, 0.4553, 0.4258], std=[0.2439, 0.2375, 0.2457]) # Normalize each image, numbers because of function courtesy of chatgpt\n",
    "        ])\n",
    "        train_set = datasets.Imagenette(root='./data', split='train', download=download, size='full', transform=imagenette_transform)\n",
    "        test_set = datasets.Imagenette(root='./data', split='val', download=False, size='full', transform=imagenette_transform)\n",
    "    \n",
    "    # If we want a validation set of a given size, take it from train set\n",
    "    if validation_size is not None:\n",
    "        # These will both be of the torch.utils.data.Subset type (not the Dataset type), and are basically just mappings of indices\n",
    "        # This does not matter when we make Dataloaders of them, however\n",
    "        if dataset_name != 'imagenette':\n",
    "            train_set, validation_set = torch.utils.data.random_split(train_set, [1-validation_size, validation_size])\n",
    "\n",
    "        # In the case of imagenette, the 'test set' is already a pretty big validation set, so we'll use that to create the test set instead\n",
    "        else:\n",
    "            validation_set, test_set = torch.utils.data.random_split(test_set, [validation_size, 1-validation_size])\n",
    "\n",
    "    if v:\n",
    "        print(f\"There are {len(train_set)} examples in the training set\")\n",
    "        print(f\"There are {len(test_set)} examples in the test set \\n\")\n",
    "\n",
    "        print(f\"Image shape is: {train_set[0][0].shape}, label example is {train_set[0][1]}\")\n",
    "\n",
    "    return train_set, validation_set, test_set\n",
    "\n",
    "# collate function just to cast to device, same as in week_3 exercises\n",
    "def collate_fn(batch):\n",
    "    return tuple(x_.to(device) for x_ in default_collate(batch))"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color: #E6E6E6;\">\n",
    "<span style=\"background-color: #545454; padding:8px; display:block; border-left:4px solid #4682b4\">\n",
    "\n",
    "### Task 1 - Theoretical questions\n",
    "\n",
    "These questions are meant to test your general knowledge of CNN's, feel free to contact or write the TA's if you have any questions about any of them\n",
    "\n",
    "\n",
    "#### **1.1. What is the reason we add MaxPooling or AveragePooling in CNN's?**\n",
    "\n",
    "<span style=\"background-color: #00590D; padding:8px; display:block; border-left:4px solid #4682b4\">\n",
    "\n",
    "So, pooling reduces spatial size of feature maps, by down samlpling. This in turn lowers the number of parameters and computations in layers, increasing the speed and memory utilization of the NN. Pooling also increases the invariance of the model, making it less succebtible to other transformations in the input. pooling can also help with noise reduction by emphasizing important features while discarding small variations/noise. It helps generalize the model, and makes it not focus on small pixels, but the overall input. \n",
    "\n",
    "</span>\n",
    "\n",
    "#### **1.2. In the [VGG paper](https://arxiv.org/pdf/1409.1556), last paragraph of 'training', page 4, they mention images being randomly cropped after being rescaled. Why do you think they crop images only *after* rescaling them?**\n",
    "\n",
    "<span style=\"background-color: #00590D; padding:8px; display:block; border-left:4px solid #4682b4\">\n",
    "\n",
    "Well, the first thing in mind is to retain as much possible potentially important information before cropping. There is also the required input size for the network(224x224). If we did the crop first, there's a risc of distorting the image when rescaling the input. \n",
    "There is also something about $S$ in it's effect on the image size (shorter side of image = $S$). EX: image size = 800x600\n",
    "S = 256 -> 256/600 0.43 -> new image size = 344((800*0.43))x256. it is the cropped part of the image. exlpain further\n",
    "\n",
    "</span>\n",
    "\n",
    "#### **1.3. After this, they mention \"further augmenting the dataset\" by random horizontal flipping and random RGB color shift. Why do you think they do this?**\n",
    "\n",
    "<span style=\"background-color: #00590D; padding:8px; display:block; border-left:4px solid #4682b4\">\n",
    "\n",
    "Objects don't have a designated right or left, up or down direction, so to ensure there is no bias in the direction of the data, the random horizontal flipping helps the model train on varied directional inputs, making it more robust in that regard. It makes the model focus on what an object is, not where it's pointed. In the same manner, different shifts in RGB can help the model recognize different objects in different lighting scenarios, as that also varies greatly in reality (Lighting, shadows, camerafilters, hues etc) so hopefully this makes the model more colour invariant.\n",
    "These two methods makes the model rely more on shapes, textures and edges\n",
    "\n",
    "</span>\n",
    "\n",
    "#### **1.4. Why do you think they do not randomly translate images? (Translate being to move images left, right, up, down)**\n",
    "\n",
    "<span style=\"background-color: #00590D; padding:8px; display:block; border-left:4px solid #4682b4\">\n",
    "\n",
    "They do not use translations because they already use cropping with the setup of convolution and pooling. This essentially does the same thing, but in a more effective way, with the random cropping. This means it becomes redundant to also use translations.\n",
    "\n",
    "</span>\n",
    "\n",
    "#### **1.5. Which of the following classification tasks do you think is more difficult for a machine learning model, and why?**\n",
    "\n",
    "- **Telling German Shepherds (Schæferhunde) from Labradors**\n",
    "- **Telling dogs from cats**\n",
    "- **Telling horses from cars from totem poles from chainsaws**\n",
    "\n",
    "<span style=\"background-color: #00590D; padding:8px; display:block; border-left:4px solid #4682b4\">\n",
    "\n",
    "1. differentiation between dog breeds is propably the most difficult task, since there are so many similarities between these breeds, so the model must learn to differentiate the small details i.e it being more difficult\n",
    "\n",
    "2. Dogs and cats are somewhat similar in structure, but the overall differences should be more than noticiable to the system, making it somewhat easier than differentiating dog breeds, but still on the difficult side\n",
    "\n",
    "3. Differences between horses(four legged animals) and cars(inanimate vehicle) should be great, making the model able to rely more on less detailed features. Totem poles and chainsaws are also very different in structure, so the same applies.\n",
    "\n",
    "</span>\n",
    "\n",
    "#### **1.6. In real life, you often find that neural networks aren't used \"for everything\", older and often more simple models like random forest and linear regression still dominate a lot of fields. Reason a bit about why this is the case**\n",
    "\n",
    "<span style=\"background-color: #00590D; padding:8px; display:block; border-left:4px solid #4682b4\">\n",
    "\n",
    "Propably because some simple models are way more efficent in terms of computing cost, and because they simply mimic real life tendencies that are abundant. So it doesnt always justify the cost of using a NN, when a simple model does it faster and cheaper. \n",
    "Cases for NN's are where there is an abundance of high dimensional data with lots of features to weigh, usually more complicated tasks where a simple model doesn't cut it. \n",
    "\n",
    "*utputs from simple models are also easily understood/interpreted, whereas explaining the architecture and function of a NN and how it produces the output can be difficult* maybe cut\n",
    "\n",
    "There is also something to discuss in terms of sheer size of datasets for models. NN's usually require vast amounts of data, where general simple models can usually cut it with less(still, more is better, if you disregard cost)\n",
    "\n",
    "</span>\n",
    "\n",
    "\n",
    "#### **1.7. The VGG16-D conv layers all use the same kernel size. Come up with reasons for why you would use bigger/smaller kernel sizes**\n",
    "\n",
    "<span style=\"background-color: #00590D; padding:8px; display:block; border-left:4px solid #4682b4\">\n",
    "\n",
    "Smaller kernels focus on the small details in local patterns, possibly finding more non linearities.\n",
    "Larger kernels conversely find larger patterns and add more weights pr input channel. a 7x7 kernel adds 49 weights, while three 3x3 kernels only add 27 weights. Smaller kernels mean less parameters, which makes the model less prone to overfitting. also computationally cheaper, since there is less work pr pixel analyzed. Smaller kernels allow for more activation functions between layers.\n",
    "\n",
    "\n",
    "\n",
    "</span>\n",
    "\n",
    "</span>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color: #E6E6E6;\">\n",
    "<span style=\"background-color: #00695C; padding:8px; display:block; border-left:4px solid #4682b4\">\n",
    "\n",
    "## Boilerplate end - Your implementation work begins here:\n",
    "\n",
    "Below, you are given a working example of a CNN, not much different from the one in the exercises of week 3. Your job is to complete the implementation questions below.\n",
    "\n",
    "You do not need to do all the exercises below, or even do them in order, we will obviously only grade the ones you have done, however. Please just check off completed exercises as shown below, so we will know what to look for when grading your assignment. You can add as much text below each question as you want to either argue for your choice of implementation, discuss your results, or ask us questions, we will consider this when grading the assignment.\n",
    "\n",
    "Many tasks include a <input type=\"checkbox\"> checkbox. These are for you to check when you have written code to help solve an exercise. This is to indicate to the TA's which tasks to look for code to correct. Remember to check these if you want feedback on your code.\n",
    "\n",
    "For your convenience, we reccommend implementing two models: One bigger for the VGG16-D exercises, meant to be used only with images from the Imagenette dataset, and one smaller, which can also take the other datasets. The model already implemented below should fill the role of the latter.\n",
    "\n",
    "Finally, if you're not able to train the VGG16-D model because it is too big, you can also load the weights of the model using the funciton implemented for exactly that. We do, however, reccommend training it from scratch yourself, if possible.\n",
    "\n",
    "You can toggle the weight imports using the boolean `USE_PRETRAINED_WEIGHTS`. This results in a small reduction in iteration time - the real time save is that it now requires significantly fewer epochs to reach a decent accuracy. (The iteration time is roughly the same since the forward pass through the model must be computed in the same manner.)\n",
    "\n",
    "**DISCLAIMER: The weight imports will fail if your VGG16 model does not correctly correspond to the correct model architecture.**\n",
    "\n",
    "---\n",
    "\n",
    "To reiterate: The code in the cells below should already work to train a model for the MNIST dataset. **This is not the VGG16-D model, however!** To implement VGG16-D, you are suposed to copy or replace this already existing model with your own implementation.\n",
    "\n",
    "</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color: #E6E6E6;\">\n",
    "<span style=\"background-color: #545454; padding:8px; display:block; border-left:4px solid #4682b4\">\n",
    "\n",
    "### 💻 Task 2 - Implementation\n",
    "\n",
    "Assume all tasks below here have the 💻 tag - they all require some degree of implementation work. \n",
    "\n",
    "Many tasks include a <input type=\"checkbox\"> checkbox. These are for you to check when you have written code to help solve an exercise. This is to indicate to the TA's which tasks to look for code to correct. Remember to check these if you want feedback on your code.\n",
    "\n",
    "Remember, green boxes are for providing explanations. They are not mandatory to fill out, but if you want feedback for your reasoning, write in them.\n",
    "\n",
    "**HINT: If it is taking waaaaay to long to train with imagenette, you can go around what they do in the paper (resizing to 256 and thereafter, cropping to 224), by reducing either of these to other values. Much of the training time when using imagenette can reasonably be attributed to simply having high-resolution images. (Do note that reducing resolution is likely to reduce model performance as well)**\n",
    "\n",
    "#### **2.1. Implement the layer structure of VGG16-D by following either this [Medium article](https://medium.com/@mygreatlearning/everything-you-need-to-know-about-vgg16-7315defb5918) (fairly easy), or the [official paper](https://arxiv.org/pdf/1409.1556) (slightly harder)**\n",
    "\n",
    "\n",
    "**Note: The original VGG16-D layer structure is meant to be used with $224 \\times 224$ sized images, only the  imagenette dataset in this notebook has this, not the MNIST**\n",
    "\n",
    "#### **2.2. Overfitting Analysis**\n",
    "- <input type=\"checkbox\"> **Can you make the VGG16-D model overfit to the imagenette dataset? If not, what about another dataset?**\n",
    "\n",
    "\n",
    "<span style=\"background-color: #00590D; padding:8px; display:block; border-left:4px solid #4682b4\">\n",
    "\n",
    "Overfitting is when our model has learned too well from the training data, and thus performs very well on training data, but poorly on testing data. The model has focused too much on the noise present in the data, and thus cannot correctly classify the test data. Underfitting would be the model learning too little, and thus performing poorly on both training and test data.\n",
    "\n",
    "so if we use very little training data, the model should be very good at recognizing those patterns, but will fail more often when testing on new data. Alternatively we could remove some of our regularization methods. If we remove dropout(i.e the random turnoff of neutrons, which hinders the model in learning exact patterns.) the model is more prone to retain exact patterns from the training data. \n",
    "\n",
    "If we remove weight decay(i.e the model lowering values of large weights, leading to less memorization) the model emphasizes more on the learned weights from training, making it less flexible for new data. If we remove the random cropping and colour shifting the models is also less resistant to learning the underlying patterns in edges and shapes, making it decent at training and bad at testing. \n",
    "\n",
    "if we increase the amount of parameters, there is more information to retain from the training and it gets more rooted in the training set. paired with a small training data set makes the model more fixed in the training patterns.\n",
    "\n",
    "\n",
    "\n",
    "</span>\n",
    "\n",
    "- <input type=\"checkbox\"> **Can you change the amount of dropout to increase or decrease the rate of overfitting?**\n",
    "\n",
    "\n",
    "\n",
    "<span style=\"background-color: #00590D; padding:8px; display:block; border-left:4px solid #4682b4\">\n",
    "\n",
    "if we decrease dropout, it should should increase the risk of the model being rooted in training data(memorizing it), and then failing on new testing data. If we increase dropout too much we don't allow the model to retain any useful patterns, which makes the model underfit.\n",
    "from the paper they use a dropoutrate of 0.5(half the neurons are deactivated randomly)  \n",
    "\n",
    "\n",
    "</span>\n",
    "\n",
    "- <input type=\"checkbox\"> **Can you make the smaller model overfit to any of its datasets? Is it harder or easier? Explain your answer**\n",
    "\n",
    "\n",
    "<span style=\"background-color: #00590D; padding:8px; display:block; border-left:4px solid #4682b4\">\n",
    "\n",
    "\n",
    "If we just use default settings for the smaller model it overfits. If we check the architecture of the smaller model we can see that the models lacks regularizations. there is no dropout, no regularization of the data. the model does use weight decay(adam), and there is also way less convolutional layers and pooling layers. the model is simpler, and more suitable for prototyping simpler tasks, rather than image analaysis\n",
    "\n",
    "\n",
    "</span>\n",
    "\n",
    "#### **2.3. Hyperparameter Optimization**\n",
    "- <input type=\"checkbox\"> Try to improve the test accuracy of either of your models by changing some of they hyperparameters. To make it easier, try to keep detailed results of your experimental setups and your preliminary results. Argue for your changes. Examples of possible changes are shown below:\n",
    "- <input type=\"checkbox\"> Add more/fewer kernels\n",
    "- <input type=\"checkbox\"> Add more/less dropout\n",
    "- <input type=\"checkbox\"> Add [BatchNorm](https://pytorch.org/docs/stable/generated/torch.nn.BatchNorm2d.html)\n",
    "- <input type=\"checkbox\"> Change the intial transform when loading the data to make larger/smaller images\n",
    "\n",
    "<span style=\"background-color: #00590D; padding:8px; display:block; border-left:4px solid #4682b4\">\n",
    "\n",
    "For the Simple model, making each convolutional layer have consistent padding(default for conv2d is padding=0). whithoud padding we turn the output size smaller, and then we lose details. And if we add more conv layers, we increase the amount of information we can get from the inputs. it goes: lower layers: edges and simple textures, middle layers: shapes and parts of objects, deep layers: learning objects as a whole. the more layers we add of convolution the more the network learns spatial patterns. Then if we add some dropout layers we excpect the model to be more resilient to being stuck in the patterns of the training data, and more succeptible to learn the overall trends. \n",
    "we see from the plot that we have made the model less prone to overfitting, though there is only a slight difference in actual performance. For that we could further augment the model architecture, and augment the data more before we put it through the network.\n",
    "\n",
    "</span>\n",
    "\n",
    "\n",
    "#### **2.4. Performance Optimization**\n",
    "- <input type=\"checkbox\"> Change the model (or any other code in the whole script) to make either training, inference, or both, as quick as possible, while still keeping a reasonable test accuracy. Explain what you did to achieve this.\n",
    "\n",
    "<span style=\"background-color: #00590D; padding:8px; display:block; border-left:4px solid #4682b4\">\n",
    "\n",
    "#changes from simpleVGG: removed layers from the fully connected part, and added a batchnorm layer after each conv layer. Also added adaptive avg pooling to reduce feature map size before flattening.\n",
    "this should reduce parameters(lower computation time) and increase speed of convergence.\n",
    "Batchnorm was added after convolutions to spped up training and stabilizing gradients, so we hopefully need less epochs.\n",
    "used less channels deeper layers(256 -> 128) to reduce parameters and computation time.\n",
    "AMP is used for faster training with less memory usage.\n",
    "\n",
    "Average runtimes for the individual models are listed below:\n",
    "- VGG16: ~40 seconds pr epoch\n",
    "- OG (original model): ~19 seconds pr epoch\n",
    "- Simple VGG: ~33 seconds pr epoch\n",
    "- Fast Simple VGG: ~44 seconds pr epoch\n",
    "\n",
    "It is worth noting that the fast simple VGG surprisingly did not perform at a faster rate but ended up with similar test accuracy, which does not make sense based on the convolution layer, but might be related to the evaluation method. The simple VGG model output an incorrect validation accuracy of constant 0.1. This will have to be further looked into as the issue with the simple VGG model was not found.\n",
    "\n",
    "</span>\n",
    "\n",
    "\n",
    "#### **2.5. Error Analysis**\n",
    "- <input type=\"checkbox\"> During evaluation, extract some images which were not correctly classified, plot these images and reason about what made them hard-to-classify\n",
    "\n",
    "\n",
    "<span style=\"background-color: #00590D; padding:8px; display:block; border-left:4px solid #4682b4\">\n",
    "\n",
    "There were several factors that made some images difficult to classify. For example, some backgrounds looked similar to those of another class, objects appeared in unusual poses or were cropped, and certain classes looked very similar to each other. Additional challenges came from images that were blurry, dark, or contained very small objects, leaving too little clear information to classify them correctly. These were some of the key factors that contributed to the difficulty in classifying certain images. \n",
    "\n",
    "</span>\n",
    "\n",
    "</span>"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from torchvision.utils import make_grid\n",
    "from pathlib import Path\n",
    "\n",
    "def get_misclassified(model, dataloader, n=10, device=None, return_tensors=False):\n",
    "    \"\"\"\n",
    "    Collect up to n misclassified examples from dataloader.\n",
    "    Returns lists: images (tensors), preds (ints), targets (ints), dataset_indices (ints), paths (or None)\n",
    "    - model: trained PyTorch model\n",
    "    - dataloader: DataLoader for eval/test set\n",
    "    - n: number of misclassified examples to collect\n",
    "    - device: 'cuda' or 'cpu' or None (use model.device or cuda if available)\n",
    "    - return_tensors: if True, returned images are tensors; otherwise converted to CPU numpy arrays (CHW -> HWC)\n",
    "    \"\"\" #ty gpt\n",
    "    if device is None:\n",
    "        device = next(model.parameters()).device if any(True for _ in model.parameters()) else torch.device('cpu')\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "\n",
    "    imgs = []\n",
    "    preds = []\n",
    "    targets = []\n",
    "    idxs = []\n",
    "    paths = []\n",
    "\n",
    "    dataset_index = 0\n",
    "    with torch.no_grad():\n",
    "        for batch in dataloader:\n",
    "            # allow dataloader returning (inputs, targets) or (inputs, targets, paths)\n",
    "            if len(batch) == 2:\n",
    "                inputs, labels = batch\n",
    "                batch_paths = None\n",
    "            else:\n",
    "                # common pattern: (inputs, labels, paths) OR dataset returns more things\n",
    "                inputs, labels, *rest = batch\n",
    "                batch_paths = rest[0] if len(rest) > 0 else None\n",
    "\n",
    "            inputs = inputs.to(device, non_blocking=True)\n",
    "            labels = labels.to(device, non_blocking=True)\n",
    "\n",
    "            logits = model(inputs)\n",
    "            batch_preds = torch.argmax(logits, dim=1)\n",
    "\n",
    "            # find misclassified indices in this batch\n",
    "            mask = (batch_preds != labels)\n",
    "            if mask.any():\n",
    "                mis_idx_in_batch = mask.nonzero(as_tuple=False).squeeze(1)\n",
    "                for k in mis_idx_in_batch:\n",
    "                    if len(imgs) >= n:\n",
    "                        break\n",
    "                    k = int(k)\n",
    "                    # global dataset index of this sample (approx): running dataset_index + k\n",
    "                    global_idx = dataset_index + k\n",
    "                    img_tensor = inputs[k].detach().cpu()\n",
    "                    imgs.append(img_tensor)\n",
    "                    preds.append(int(batch_preds[k].cpu().item()))\n",
    "                    targets.append(int(labels[k].cpu().item()))\n",
    "                    idxs.append(global_idx)\n",
    "                    # try to get path if available\n",
    "                    if batch_paths is not None:\n",
    "                        try:\n",
    "                            # Some dataloaders give paths as bytes/strings per sample\n",
    "                            p = batch_paths[k]\n",
    "                            paths.append(str(p))\n",
    "                        except Exception:\n",
    "                            paths.append(None)\n",
    "                    else:\n",
    "                        paths.append(None)\n",
    "            dataset_index += inputs.size(0)\n",
    "\n",
    "            if len(imgs) >= n:\n",
    "                break\n",
    "\n",
    "    # convert to numpy arrays (HWC) if requested\n",
    "    if not return_tensors:\n",
    "        imgs = [img.numpy().transpose(1, 2, 0) for img in imgs]\n",
    "\n",
    "    return imgs, preds, targets, idxs, paths\n",
    "\n",
    "# helper: display misclassified in a grid (handles tensors or numpy HWC)\n",
    "def show_misclassified(imgs, preds, targets, class_names=None, ncols=5, figsize=(12,8), unnormalize=None):\n",
    "    \"\"\"\n",
    "    imgs: list of tensors (C,H,W) or numpy arrays (H,W,C) in [0,1] or normalized.\n",
    "    unnormalize: function(tensor_or_array) -> array in [0,1] (optional). Example below.\n",
    "    \"\"\"\n",
    "    n = len(imgs)\n",
    "    ncols = min(ncols, n)\n",
    "    nrows = (n + ncols - 1) // ncols\n",
    "    plt.figure(figsize=figsize)\n",
    "    for i, img in enumerate(imgs):\n",
    "        plt.subplot(nrows, ncols, i+1)\n",
    "        if torch.is_tensor(img):\n",
    "            arr = img.cpu().numpy().transpose(1,2,0)\n",
    "        else:\n",
    "            arr = img\n",
    "        if unnormalize is not None:\n",
    "            arr = unnormalize(arr)\n",
    "        # clip and handle single channel\n",
    "        arr = arr.clip(0, 1)\n",
    "        if arr.shape[2] == 1: \n",
    "            plt.imshow(arr[:, :, 0], cmap='gray')\n",
    "        else:\n",
    "            plt.imshow(arr)\n",
    "        title = f\"prediction={preds[i]} / target={targets[i]}\"\n",
    "        if class_names is not None:\n",
    "            title = f\"prediction={class_names[preds[i]]}\\n target={class_names[targets[i]]}\"\n",
    "        plt.title(title, fontsize=8)\n",
    "        plt.axis('off')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# helper: save misclassified images to folder\n",
    "def save_misclassified(imgs, preds, targets, out_dir=\"misclassified\", prefix=\"mis\", unnormalize=None):\n",
    "    Path(out_dir).mkdir(parents=True, exist_ok=True)\n",
    "    for i, img in enumerate(imgs):\n",
    "        if torch.is_tensor(img):\n",
    "            arr = img.cpu().numpy().transpose(1,2,0)\n",
    "        else:\n",
    "            arr = img\n",
    "        if unnormalize is not None:\n",
    "            arr = unnormalize(arr)\n",
    "        arr = (arr.clip(0,1) * 255).astype('uint8')\n",
    "        # handle grayscale\n",
    "        if arr.shape[2] == 1:\n",
    "            plt.imsave(os.path.join(out_dir, f\"{prefix}_{i}_p{preds[i]}_t{targets[i]}.png\"), arr[:,:,0], cmap='gray')\n",
    "        else:\n",
    "            plt.imsave(os.path.join(out_dir, f\"{prefix}_{i}_p{preds[i]}_t{targets[i]}.png\"), arr)\n",
    "\n",
    "\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "class OG(torch.nn.Module):\n",
    "    def __init__(self, num_classes, in_channels=1, features_fore_linear=25088, dataset=None):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Helper hyperparameters to keep track of VGG16 architecture\n",
    "        conv_stride = ...\n",
    "        pool_stride = ...\n",
    "        conv_kernel = ...\n",
    "        pool_kernel = ...\n",
    "        dropout_probs = ...\n",
    "        optim_momentum = ...\n",
    "        weight_decay = ...\n",
    "        learning_rate = ...\n",
    "\n",
    "        # Define features and classifier each individually, this is how the VGG16-D model is orignally defined\n",
    "        self.features = torch.nn.Sequential(\n",
    "            nn.Conv2d(in_channels=in_channels, out_channels=32, kernel_size=3, padding=1), # dim = in\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3),\n",
    "            nn.MaxPool2d(kernel_size=2),\n",
    "            nn.ReLU(),\n",
    "            nn.Flatten(),\n",
    "        ).to(device)\n",
    "        \n",
    "        self.classifier = torch.nn.Sequential(\n",
    "            nn.Linear(in_features=features_fore_linear, out_features=600),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(in_features=600, out_features=120),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(in_features=120, out_features=num_classes)\n",
    "        ).to(device)\n",
    "        \n",
    "        # In the paper, they mention updating towards the 'multinomial logistic regression objective'\n",
    "        # As can be read in Bishop p. 159, taking the logarithm of this equates to the cross-entropy loss function.\n",
    "        self.criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "        # Optimizer - For now just set to Adam to test the implementation\n",
    "        self.optim = torch.optim.Adam(list(self.features.parameters()) + list(self.classifier.parameters()), lr=0.001)\n",
    "        # self.optim = torch.optim.SGD(list(self.features.parameters()) + list(self.classifier.parameters()), lr=learning_rate, momentum=optim_momentum, weight_decay=weight_decay)\n",
    "\n",
    "        self.dataset = dataset\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        return self.classifier(x)\n",
    "\n",
    "    def train_model(self, train_dataloader, epochs=1, val_dataloader=None):\n",
    "        \n",
    "        # Call .train() on self to turn on dropout\n",
    "        self.train()\n",
    "\n",
    "        # To hold accuracy during training and testing\n",
    "        train_accs = []\n",
    "        test_accs = []\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "            \n",
    "            epoch_acc = 0\n",
    "\n",
    "            for inputs, targets in tqdm(train_dataloader):\n",
    "                logits = self(inputs)\n",
    "                loss = self.criterion(logits, targets)\n",
    "                loss.backward()\n",
    "\n",
    "                self.optim.step()\n",
    "                self.optim.zero_grad()\n",
    "\n",
    "                # Keep track of training accuracy\n",
    "                epoch_acc += (torch.argmax(logits, dim=1) == targets).sum().item()\n",
    "            train_accs.append(epoch_acc / len(train_dataloader.dataset))\n",
    "\n",
    "            # If val_dataloader, evaluate after each epoch\n",
    "            if val_dataloader is not None:\n",
    "                # Turn off dropout for testing\n",
    "                self.eval()\n",
    "                acc = self.eval_model(val_dataloader)\n",
    "                test_accs.append(acc)\n",
    "                print(f\"Epoch {epoch} validation accuracy: {acc}\")\n",
    "                # turn on dropout after being done\n",
    "                self.train()\n",
    "        \n",
    "        return train_accs, test_accs\n",
    "\n",
    "    def eval_model(self, test_dataloader):\n",
    "        \n",
    "        self.eval()\n",
    "        total_acc = 0\n",
    "\n",
    "        for input_batch, label_batch in test_dataloader:\n",
    "            logits = self(input_batch)\n",
    "\n",
    "            total_acc += (torch.argmax(logits, dim=1) == label_batch).sum().item()\n",
    "\n",
    "        total_acc = total_acc / len(test_dataloader.dataset)\n",
    "\n",
    "        return total_acc\n",
    "\n",
    "    def predict(self, img_path):\n",
    "        img = PIL.Image.open(img_path)\n",
    "        img = self.dataset.dataset.transform(img)\n",
    "        classification = torch.argmax(self(img.unsqueeze(dim=0)), dim=1)\n",
    "        return img, classification\n",
    "    \n",
    "    def predict_random(self, num_predictions=16):\n",
    "        \"\"\"\n",
    "        Plot random images from own given dataset\n",
    "        \"\"\"\n",
    "        random_indices = np.random.choice(len(self.dataset)-1, num_predictions, replace=False)\n",
    "        classifcations = []\n",
    "        labels = []\n",
    "        images = []\n",
    "        for idx in random_indices:\n",
    "            img, label = self.dataset.__getitem__(idx)\n",
    "\n",
    "            classifcation = torch.argmax(self(img.unsqueeze(dim=0)), dim=1)\n",
    "\n",
    "            classifcations.append(classifcation)\n",
    "            labels.append(label)\n",
    "            images.append(img)\n",
    "\n",
    "        return classifcations, labels, images\n",
    "\n",
    "\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "class simpleVGG(torch.nn.Module):\n",
    "    def __init__(self, num_classes, in_channels=1, features_fore_linear=25088, dataset=None):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Helper hyperparameters to keep track of VGG16 architecture\n",
    "        conv_stride = 1\n",
    "        pool_stride = 2\n",
    "        conv_kernel = (3,3)\n",
    "        pool_kernel = (2,2)\n",
    "        dropout_probs = 0.3\n",
    "        optim_momentum = 0.9\n",
    "        weight_decay = 5*10**(-4)\n",
    "        learning_rate = 10**(-2)\n",
    "\n",
    "        # Define features and classifier each individually, this is how the VGG16-D model is orignally defined\n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, 32, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(32, 64, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(2, 2),\n",
    "\n",
    "            nn.Conv2d(64, 128, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(128, 128, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(2, 2),\n",
    "\n",
    "            nn.Conv2d(128, 256, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(256, 256, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(2, 2),\n",
    "\n",
    "            nn.Flatten()).to(device)\n",
    "        \n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(features_fore_linear, 1024),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(1024, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(256, num_classes)).to(device)\n",
    "\n",
    "        # In the paper, they mention updating towards the 'multinomial logistic regression objective'\n",
    "        # As can be read in Bishop p. 159, taking the logarithm of this equates to the cross-entropy loss function.\n",
    "        self.criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "        # Optimizer - For now just set to Adam to test the implementation\n",
    "        self.optim = torch.optim.Adam(\n",
    "        self.parameters(),\n",
    "        lr=1e-3,\n",
    "        weight_decay=1e-4)\n",
    "        # self.optim = torch.optim.SGD(list(self.features.parameters()) + list(self.classifier.parameters()), lr=learning_rate, momentum=optim_momentum, weight_decay=weight_decay)\n",
    "\n",
    "        self.dataset = dataset\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        return self.classifier(x)\n",
    "\n",
    "    def train_model(self, train_dataloader, epochs=1, val_dataloader=None):\n",
    "        \n",
    "        # Call .train() on self to turn on dropout\n",
    "        self.train()\n",
    "\n",
    "        # To hold accuracy during training and testing\n",
    "        train_accs = []\n",
    "        test_accs = []\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "            \n",
    "            epoch_acc = 0\n",
    "\n",
    "            for inputs, targets in tqdm(train_dataloader):\n",
    "                logits = self(inputs)\n",
    "                loss = self.criterion(logits, targets)\n",
    "                loss.backward()\n",
    "\n",
    "                self.optim.step()\n",
    "                self.optim.zero_grad()\n",
    "\n",
    "                # Keep track of training accuracy\n",
    "                epoch_acc += (torch.argmax(logits, dim=1) == targets).sum().item()\n",
    "            train_accs.append(epoch_acc / len(train_dataloader.dataset))\n",
    "\n",
    "            # If val_dataloader, evaluate after each epoch\n",
    "            if val_dataloader is not None:\n",
    "                # Turn off dropout for testing\n",
    "                self.eval()\n",
    "                acc = self.eval_model(val_dataloader)\n",
    "                test_accs.append(acc)\n",
    "                print(f\"Epoch {epoch} validation accuracy: {acc}\")\n",
    "                # turn on dropout after being done\n",
    "                self.train()\n",
    "        \n",
    "        return train_accs, test_accs\n",
    "\n",
    "    def eval_model(self, test_dataloader):\n",
    "        \n",
    "        self.eval()\n",
    "        total_acc = 0\n",
    "\n",
    "        for input_batch, label_batch in test_dataloader:\n",
    "            logits = self(input_batch)\n",
    "\n",
    "            total_acc += (torch.argmax(logits, dim=1) == label_batch).sum().item()\n",
    "\n",
    "        total_acc = total_acc / len(test_dataloader.dataset)\n",
    "\n",
    "        return total_acc\n",
    "\n",
    "    def predict(self, img_path):\n",
    "        img = PIL.Image.open(img_path)\n",
    "        img = self.dataset.dataset.transform(img)\n",
    "        classification = torch.argmax(self(img.unsqueeze(dim=0)), dim=1)\n",
    "        return img, classification\n",
    "    \n",
    "    def predict_random(self, num_predictions=16):\n",
    "        \"\"\"\n",
    "        Plot random images from own given dataset\n",
    "        \"\"\"\n",
    "        random_indices = np.random.choice(len(self.dataset)-1, num_predictions, replace=False)\n",
    "        classifcations = []\n",
    "        labels = []\n",
    "        images = []\n",
    "        for idx in random_indices:\n",
    "            img, label = self.dataset.__getitem__(idx)\n",
    "\n",
    "            classifcation = torch.argmax(self(img.unsqueeze(dim=0)), dim=1)\n",
    "\n",
    "            classifcations.append(classifcation)\n",
    "            labels.append(label)\n",
    "            images.append(img)\n",
    "\n",
    "        return classifcations, labels, images\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "from torch.cuda import amp\n",
    "def conv_block(in_channels, out_channels, k=3, padding=1):\n",
    "    return nn.Sequential(\n",
    "        nn.Conv2d(in_channels, out_channels, kernel_size=k, padding=padding, bias=False),\n",
    "        nn.BatchNorm2d(out_channels),\n",
    "        nn.ReLU(inplace=True))\n",
    "\n",
    "class fastsimpleVGG(torch.nn.Module):                                  #512x7x7\n",
    "    def __init__(self, num_classes, in_channels=1, features_fore_linear=25088, dataset=None): #(self,num_classes, in_channels=1, base_channels=32, dataset = None):\n",
    "        super().__init__()\n",
    "        base_channels = 32\n",
    "\n",
    "        self.features = nn.Sequential(\n",
    "            conv_block(in_channels, base_channels), \n",
    "            conv_block(base_channels, base_channels),\n",
    "            nn.MaxPool2d(2, 2),                     \n",
    "\n",
    "\n",
    "            conv_block(base_channels, base_channels*2), \n",
    "            conv_block(base_channels*2, base_channels*2),\n",
    "            nn.MaxPool2d(2, 2),                         \n",
    "\n",
    "\n",
    "            conv_block(base_channels*2, base_channels*4), \n",
    "            nn.AdaptiveAvgPool2d((1,1)), # adaptiveavg pool \n",
    "            nn.Flatten()\n",
    "        ).to(device)\n",
    "\n",
    "\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(in_features=128, out_features=600),  \n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(in_features=600, out_features=num_classes)\n",
    "        ).to(device)\n",
    "\n",
    "        self.criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "        #self.optim = torch.optim.Adam(self.features.parameters()) + list(self.classifier.parameters()), lr=1e-3, weight_decay=1e-4))\n",
    "        \n",
    "        self.optim = torch.optim.Adam(list(self.features.parameters()) + list(self.classifier.parameters()), lr=0.001, weight_decay=1e-4)\n",
    "\n",
    "        self.dataset = dataset\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        return self.classifier(x)\n",
    "    \n",
    "    def train_model(self, train_dataloader, epochs=10, val_dataloader=None, device='cuda'):\n",
    "        self.train()\n",
    "\n",
    "        self.to(device)\n",
    "        scaler = amp.GradScaler()\n",
    "        scheduler = torch.optim.lr_scheduler.StepLR(self.optim, step_size=8, gamma=0.1)\n",
    "        train_accs, test_accs = [], []\n",
    "\n",
    "        # speed trick\n",
    "        torch.backends.cudnn.benchmark = True\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "            self.train()\n",
    "            running_correct = 0\n",
    "            total = 0\n",
    "            pbar = tqdm(train_dataloader, desc=f\"Epoch {epoch}\")\n",
    "            for inputs, targets in pbar:\n",
    "                inputs = inputs.to(device, non_blocking=True)\n",
    "                targets = targets.to(device, non_blocking=True)\n",
    "\n",
    "                with amp.autocast():\n",
    "                    logits = self(inputs)\n",
    "                    loss = self.criterion(logits, targets)\n",
    "\n",
    "                self.optim.zero_grad(set_to_none=True)\n",
    "                scaler.scale(loss).backward()\n",
    "                scaler.step(self.optim)\n",
    "                scaler.update()\n",
    "\n",
    "                preds = logits.argmax(dim=1)\n",
    "                running_correct += (preds == targets).sum().item()\n",
    "                total += targets.size(0)\n",
    "                pbar.set_postfix(loss=loss.item(), acc=running_correct/total)\n",
    "\n",
    "            train_accs.append(running_correct / len(train_dataloader.dataset))\n",
    "\n",
    "            if val_dataloader is not None:\n",
    "                self.eval()\n",
    "                test_acc = self.eval_model(val_dataloader, device=device)\n",
    "                test_accs.append(test_acc)\n",
    "                print(f\"  Test acc: {test_acc:.4f}\")\n",
    "\n",
    "            scheduler.step()\n",
    "\n",
    "        return train_accs, test_accs\n",
    "    \n",
    "    def eval_model(self, test_dataloader, device='cuda'):\n",
    "        self.to(device)\n",
    "        self.eval()\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        # inference_mode is slightly faster than no_grad on recent PyTorch\n",
    "        with torch.inference_mode():\n",
    "            for inputs, targets in test_dataloader:\n",
    "                inputs = inputs.to(device, non_blocking=True)\n",
    "                targets = targets.to(device, non_blocking=True)\n",
    "                with amp.autocast():\n",
    "                    logits = self(inputs)\n",
    "                preds = logits.argmax(dim=1)\n",
    "                correct += (preds == targets).sum().item()\n",
    "                total += targets.size(0)\n",
    "        return correct / total\n",
    "\n",
    "#changes from simpleVGG: removed layers from the fully connected part, and added a batchnorm layer after each conv layer. Also added adaptive avg pooling to reduce feature map size before flattening.\n",
    "# this should reduce parameters(lower computation time) and increase speed of convergence.\n",
    "# Batchnorm was added after convolutions to spped up training and stabilizing gradients, so we hopefully need less epochs.\n",
    "# used less channels deeper layers(256 -> 128) to reduce parameters and computation time.\n",
    "# AMP is used for faster training with less memory usage."
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "class VGG16(torch.nn.Module):\n",
    "    def __init__(self, num_classes, in_channels=3, features_fore_linear=512*7*7, dataset=None):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Helper hyperparameters to keep track of VGG16 architecture\n",
    "        conv_stride = 1\n",
    "        pool_stride = 2\n",
    "        conv_kernel = (3,3)\n",
    "        pool_kernel = (2,2)\n",
    "        dropout_probs = 0.5\n",
    "        optim_momentum = 0.9\n",
    "        weight_decay = 5*10**(-4)\n",
    "        learning_rate = 10**(-2)\n",
    "\n",
    "        # Define features and classifier each individually, this is how the VGG16-D model is originally defined\n",
    "        self.features = torch.nn.Sequential(\n",
    "            nn.Conv2d(in_channels=in_channels, out_channels=64, kernel_size=conv_kernel, padding=1, stride=conv_stride), nn.ReLU(),\n",
    "            nn.Conv2d(in_channels=64, out_channels=64, kernel_size=conv_kernel, padding=1), nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=pool_kernel, stride=pool_stride),\n",
    "\n",
    "            nn.Conv2d(in_channels=64, out_channels=128, kernel_size=conv_kernel, padding=1, stride=conv_stride), nn.ReLU(),\n",
    "            nn.Conv2d(in_channels=128, out_channels=128, kernel_size=conv_kernel, padding=1, stride=conv_stride), nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=pool_kernel, stride=pool_stride),\n",
    "\n",
    "            nn.Conv2d(in_channels=128, out_channels=256, kernel_size=conv_kernel, padding=1, stride=conv_stride), nn.ReLU(),\n",
    "            nn.Conv2d(in_channels=256, out_channels=256, kernel_size=conv_kernel, padding=1, stride=conv_stride), nn.ReLU(),\n",
    "            nn.Conv2d(in_channels=256, out_channels=256, kernel_size=conv_kernel, padding=1, stride=conv_stride), nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=pool_kernel, stride=pool_stride),\n",
    "\n",
    "            nn.Conv2d(in_channels=256, out_channels=512, kernel_size=conv_kernel, padding=1, stride=conv_stride), nn.ReLU(),\n",
    "            nn.Conv2d(in_channels=512, out_channels=512, kernel_size=conv_kernel, padding=1, stride=conv_stride), nn.ReLU(),\n",
    "            nn.Conv2d(in_channels=512, out_channels=512, kernel_size=conv_kernel, padding=1, stride=conv_stride), nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=pool_kernel, stride=pool_stride),\n",
    "\n",
    "            nn.Conv2d(in_channels=512, out_channels=512, kernel_size=conv_kernel, padding=1, stride=conv_stride), nn.ReLU(),\n",
    "            nn.Conv2d(in_channels=512, out_channels=512, kernel_size=conv_kernel, padding=1, stride=conv_stride), nn.ReLU(),\n",
    "            nn.Conv2d(in_channels=512, out_channels=512, kernel_size=conv_kernel, padding=1, stride=conv_stride), nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=pool_kernel, stride=pool_stride),\n",
    "            nn.Flatten(),\n",
    "        ).to(device)\n",
    "        \n",
    "        self.classifier = torch.nn.Sequential(\n",
    "            nn.Linear(in_features=features_fore_linear, out_features=4096), nn.ReLU(True), nn.Dropout(dropout_probs),\n",
    "            nn.Linear(in_features=4096, out_features=4096), nn.ReLU(True), nn.Dropout(dropout_probs),\n",
    "            nn.Linear(in_features=4096, out_features=num_classes)\n",
    "        ).to(device)\n",
    "        \n",
    "        # In the paper, they mention updating towards the 'multinomial logistic regression objective'\n",
    "        # As can be read in Bishop p. 159, taking the logarithm of this equates to the cross-entropy loss function.\n",
    "        self.criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "        # Optimizer - For now just set to Adam to test the implementation\n",
    "        self.optim = torch.optim.SGD(list(self.features.parameters()) + list(self.classifier.parameters()), lr=learning_rate, momentum=optim_momentum, weight_decay=weight_decay)\n",
    "\n",
    "        self.dataset = dataset\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        return self.classifier(x)\n",
    "\n",
    "    def train_model(self, train_dataloader, epochs=1, val_dataloader=None):\n",
    "        \n",
    "        # Call .train() on self to turn on dropout\n",
    "        self.train()\n",
    "\n",
    "        # To hold accuracy during training and testing\n",
    "        train_accs = []\n",
    "        test_accs = []\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "            \n",
    "            epoch_acc = 0\n",
    "\n",
    "            for inputs, targets in tqdm(train_dataloader):\n",
    "                logits = self(inputs)\n",
    "                loss = self.criterion(logits, targets)\n",
    "                loss.backward()\n",
    "\n",
    "                self.optim.step()\n",
    "                self.optim.zero_grad()\n",
    "\n",
    "                # Keep track of training accuracy\n",
    "                epoch_acc += (torch.argmax(logits, dim=1) == targets).sum().item()\n",
    "            train_accs.append(epoch_acc / len(train_dataloader.dataset))\n",
    "\n",
    "            # If val_dataloader, evaluate after each epoch\n",
    "            if val_dataloader is not None:\n",
    "                # Turn off dropout for testing\n",
    "                self.eval()\n",
    "                acc = self.eval_model(val_dataloader)\n",
    "                test_accs.append(acc)\n",
    "                print(f\"Epoch {epoch} validation accuracy: {acc}\")\n",
    "                # turn on dropout after being done\n",
    "                self.train()\n",
    "        \n",
    "        return train_accs, test_accs\n",
    "\n",
    "    def eval_model(self, test_dataloader):\n",
    "        \n",
    "        self.eval()\n",
    "        total_acc = 0\n",
    "\n",
    "        for input_batch, label_batch in test_dataloader:\n",
    "            logits = self(input_batch)\n",
    "\n",
    "            total_acc += (torch.argmax(logits, dim=1) == label_batch).sum().item()\n",
    "\n",
    "        total_acc = total_acc / len(test_dataloader.dataset)\n",
    "\n",
    "        return total_acc\n",
    "\n",
    "    def predict(self, img_path):\n",
    "        img = PIL.Image.open(img_path)\n",
    "        img = self.dataset.dataset.transform(img)\n",
    "        classification = torch.argmax(self(img.unsqueeze(dim=0)), dim=1)\n",
    "        return img, classification\n",
    "    \n",
    "    def predict_random(self, num_predictions=16):\n",
    "        \"\"\"\n",
    "        Plot random images from own given dataset\n",
    "        \"\"\"\n",
    "        random_indices = np.random.choice(len(self.dataset)-1, num_predictions, replace=False)\n",
    "        classifications = []\n",
    "        labels = []\n",
    "        images = []\n",
    "        for idx in random_indices:\n",
    "            img, label = self.dataset.__getitem__(idx)\n",
    "\n",
    "            classification = torch.argmax(self(img.unsqueeze(dim=0)), dim=1)\n",
    "\n",
    "            classifications.append(classification)\n",
    "            labels.append(label)\n",
    "            images.append(img)\n",
    "\n",
    "        return classifications, labels, images\n",
    "\n",
    "def get_vgg_weights(model):\n",
    "    \"\"\"\n",
    "    Loads VGG16-D weights for the classifier to an already existing model\n",
    "    Also sets training to only the classifier\n",
    "    \"\"\"\n",
    "    # Load the complete VGG16 model\n",
    "    temp = torchvision.models.vgg16(weights='DEFAULT')\n",
    "\n",
    "    # Get its state dict\n",
    "    state_dict = temp.state_dict()\n",
    "\n",
    "    # Change the last layer to fit our, smaller network\n",
    "    state_dict['classifier.6.weight'] = torch.randn(10, 4096)\n",
    "    state_dict['classifier.6.bias'] = torch.randn(10)\n",
    "\n",
    "    # Apply the state dict and set the classifer (layer part) to be the only thing we train\n",
    "    model.load_state_dict(state_dict)\n",
    "\n",
    "    for param in model.features.parameters():\n",
    "        param.requires_grad = False\n",
    "\n",
    "    model.optim = torch.optim.Adam(model.classifier.parameters())\n",
    "\n",
    "\n",
    "    return model\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Get data - switch to 'mnist' or 'cifar10' for a smaller (therefore faster to train), and possibly easier dataset\n",
    "dataset_names = [\"cifar10\", \"mnist\", \"imagenette\"]\n",
    "dataset_name = dataset_names[0]  #cifar10 mnist imagenette 0, 1, 2 mnist doesnt work, output size is too small 512x0x0\n",
    "# Original imagenette resize sizes and crop sizes, these can be set lower if training is taking way too long.\n",
    "imagenette_resize_size = 224\n",
    "imagenette_crop_size = 200\n",
    "train_set, validation_set, test_set = get_dataset(dataset_name, validation_size=0.1, imagenette_resize_size=imagenette_resize_size, imagenette_crop_size=imagenette_crop_size)\n",
    "\n",
    "# Make dataloaders\n",
    "batch_size=16\n",
    "train_dataloader = torch.utils.data.DataLoader(train_set, batch_size=batch_size, shuffle=True, collate_fn=collate_fn)\n",
    "validation_dataloader = torch.utils.data.DataLoader(validation_set, batch_size=batch_size, shuffle=False, collate_fn=collate_fn)\n",
    "test_dataloader = torch.utils.data.DataLoader(test_set, batch_size=batch_size, shuffle=False, collate_fn=collate_fn)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# =============================================================================\n",
    "# TRAINING MODE SELECTION\n",
    "# =============================================================================\n",
    "# Choose your training approach:\n",
    "# - False: Train the entire model from scratch (slower, needs more epochs)\n",
    "# - True:  Use pre-trained VGG16 features + train only classifier (faster, fewer epochs)\n",
    "USE_PRETRAINED_WEIGHTS = False"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "in_channels = next(iter(train_dataloader))[0].shape[1]\n",
    "in_width_height = next(iter(train_dataloader))[0].shape[-1]\n",
    "#models: OG, simpleVGG, VGG16, fastsimpleVGG\n",
    "# Make a dummy model to find out the size before the first linear layer\n",
    "CNN_model = simpleVGG(num_classes=10, in_channels=in_channels)\n",
    "features_fore_linear = utils.get_dim_before_first_linear(CNN_model.features, in_width_height, in_channels, brain=False)\n",
    "\n",
    "# Now make true model when we know how many features we have before the first linear layer\n",
    "CNN_model = simpleVGG(num_classes=10, in_channels=in_channels, features_fore_linear=features_fore_linear, dataset=test_set)\n",
    "\n",
    "# This section loads pre-trained VGG16 weights and freezes the feature extractor\n",
    "# Only the classifier layers will be trained, making training much faster\n",
    "if USE_PRETRAINED_WEIGHTS:\n",
    "    print(\"Loading pre-trained VGG16 weights...\")\n",
    "    CNN_model = get_vgg_weights(CNN_model)\n",
    "    train_epochs = 10  # Fewer epochs needed with pre-trained features\n",
    "else:\n",
    "    print(\"Training from scratch...\")\n",
    "    train_epochs = 10  # More epochs needed when training from scratch\n",
    "\n",
    "train_accs, test_accs = CNN_model.train_model(train_dataloader, epochs=train_epochs, val_dataloader=test_dataloader)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "imgs, preds, targets, idxs, paths = get_misclassified(CNN_model, train_dataloader, n=10, device='cuda')\n",
    "\n",
    "show_misclassified(imgs, preds, targets, class_names=None,  ncols=5)\n",
    "# save_misclassified(imgs, preds, targets, out_dir=\"misclassified_examples\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# plot train and test accuracies\n",
    "plt.plot(range(train_epochs), train_accs, label='Train')\n",
    "plt.plot(range(train_epochs), test_accs, label='Test')\n",
    "\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Train and Test Accuracy over epochs, CNN model')\n",
    "plt.grid()\n",
    "plt.legend()\n",
    "if CNN_model == simpleVGG:\n",
    "    plt.savefig('simple_train_test_accuracy.png')\n",
    "elif CNN_model == VGG16:\n",
    "    plt.savefig('vgg16_train_test_accuracy.png')\n",
    "elif CNN_model == OG:\n",
    "    plt.savefig('OG_train_test_accuracy.png')\n",
    "elif CNN_model == fastsimpleVGG:\n",
    "    plt.savefig('fastsimple_train_test_accuracy.png')\n",
    "plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "attachments": {
    "image.png": {
     "image/png": "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"
    }
   },
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![image.png](attachment:image.png)\n",
    "simpleVGG16"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color: #E6E6E6;\">\n",
    "<span style=\"background-color: #545454; padding:8px; display:block; border-left:4px solid #4682b4\">\n",
    "\n",
    "### $\\star$ Exercise 3: Optional exercises\n",
    "\n",
    "These will *not* be asked as part of the exam, and are purely for your interest\n",
    "\n",
    "#### **3.1. Say a network comes with a list of class probabilities:** $\\hat{p}_1, \\hat{p}_2, \\dots \\hat{p}_N$ **when is the cross-entropy in regards to the *true* class probabilities:** $p_1, p_2, \\dots p_N$ **maximized?**\n",
    "\n",
    "<span style=\"background-color: #00590D; padding:8px; display:block; border-left:4px solid #4682b4\">\n",
    "\n",
    "$\\dots$\n",
    "\n",
    "</span>\n",
    "\n",
    "#### **3.2. When we sample from our dataloader, we sample in batches, why is this? What would be the alternatives to sampling in batches, and what impact would that have?**\n",
    "\n",
    "<span style=\"background-color: #00590D; padding:8px; display:block; border-left:4px solid #4682b4\">\n",
    "\n",
    "$\\dots$\n",
    "\n",
    "</span>\n",
    "\n",
    "#### **3.3. The \"new kid on the block' (relatively speaking) in NLP (Natural Language processing), is self-attention. Basically this is letting each word/token relate to each other word/token by a specific 'attention' value, vaguely showing how much they relate to one another... Would there be any problems in doing this for image processing by simply letting each pixel relate to each other pixel, so we can get spatial information that way instead?**\n",
    "\n",
    "\n",
    "<span style=\"background-color: #00590D; padding:8px; display:block; border-left:4px solid #4682b4\">\n",
    "\n",
    "$\\dots$\n",
    "\n",
    "</span>\n",
    "\n",
    "#### 💻 <input type=\"checkbox\"> **3.3. Figure out, and implement the type, and exact settings of the optimizer the original VGG16-D implementation used**\n",
    "\n",
    "**NOTE: The TAs tried this, and could not, for the life of us learn anything. Attempt this task at your peril!**\n",
    "\n",
    "\n",
    "#### 💻 **3.4. A hint mentions, that if training with imagenette is taking too long, you can change the transforms of them to be smaller images, and that this *might* reduce accuracy. While making this exercise, a TA ran training with the original size of $224\\times 224$ and thereafter downsized to $60\\times 60$, both models were run for for one epoch. In the case of the downsized images, accuracy was $\\approx 0.47$ as opposed to $\\approx 0.28$ without downsizing. Try to replicate these results and explain why this might be the case.**\n",
    "\n",
    "<span style=\"background-color: #00590D; padding:8px; display:block; border-left:4px solid #4682b4\">\n",
    "\n",
    "$\\dots$\n",
    "\n",
    "</span>\n",
    "\n",
    "\n",
    "#### **3.5. It's often a good idea to know the what and why of our data. Search the web for documentation on the \"imagenette\" dataset. Explain what it is, who made it, where it comes from, and what classes it contains. Why do you think it can exist in a world where imagenet exists?**\n",
    "\n",
    "<span style=\"background-color: #00590D; padding:8px; display:block; border-left:4px solid #4682b4\">\n",
    "\n",
    "$\\dots$\n",
    "\n",
    "</span>\n",
    "\n",
    "\n",
    "</span>\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MLDM",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
